{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# Assuming your data is in the \"twitter_sentiments\" folder\n",
    "train_path = \"twitter_sentiments/twitter_training.csv\"\n",
    "validation_path = \"twitter_sentiments/twitter_validation.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path, encoding='utf-8')\n",
    "validation_df = pd.read_csv(validation_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>Borderlands</th>\n",
       "      <th>Positive</th>\n",
       "      <th>im getting on borderlands and i will murder you all ,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting into borderlands and i can murder y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2401  Borderlands  Positive  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "  im getting on borderlands and i will murder you all ,  \n",
       "0  I am coming to the borders and I will kill you...     \n",
       "1  im getting on borderlands and i will kill you ...     \n",
       "2  im coming on borderlands and i will murder you...     \n",
       "3  im getting on borderlands 2 and i will murder ...     \n",
       "4  im getting into borderlands and i can murder y...     "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# PT\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# Assuming you have a column \"sentiment\" in your dataset\n",
    "class_to_label = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "\n",
    "# Drop rows with missing values in the \"Tweet content\" column\n",
    "train_df = train_df.dropna(subset=[train_df.columns[2]])\n",
    "\n",
    "# Map text labels to numerical labels\n",
    "train_df[\"label\"] = train_df.iloc[:, 2].map(class_to_label)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df.iloc[:, 2].apply(lambda x: preprocess(x) if pd.notna(x) else \"\"),  # Assuming the \"Tweet content\" column is at index 2\n",
    "    train_df[\"label\"],  # Use the new \"label\" column\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert the labels to a list of numerical values\n",
    "y_train = [class_to_label.get(label, -1) for label in y_train]\n",
    "\n",
    "# Filter out instances where the label is not in class_to_label\n",
    "X_train = [X_train[i] for i in range(len(y_train)) if y_train[i] != -1]\n",
    "y_train = [label for label in y_train if label != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for text, label in zip(X_train, y_train):\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model(**encoded_input, labels=torch.tensor([label], dtype=torch.long))\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_model\")\n",
    "\n",
    "# Testing\n",
    "# Load the fine-tuned model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"fine_tuned_model\")\n",
    "\n",
    "df = df.dropna(subset=[df.columns[0]])\n",
    "# Initialize lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the validation dataset\n",
    "for text, label in zip(X_val, y_val):\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output.logits.detach().numpy()\n",
    "    scores = softmax(scores, axis=1)\n",
    "    \n",
    "    # Get the predicted label\n",
    "    predicted_label_id = np.argmax(scores)\n",
    "    predicted_label = config.id2label[predicted_label_id]\n",
    "    \n",
    "    # Append true and predicted labels to lists\n",
    "    true_labels.append(label)\n",
    "    predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan  2.  0. ...  2.  0.  1.]\n",
      "['negative' 'positive' 'neutral' ... 'positive' 'neutral' 'neutral']\n",
      "14937\n",
      "14937\n",
      "[ True False False ... False False False]\n",
      "After removing\n",
      "12276\n",
      "12276\n",
      "[2 1 2 ... 2 1 1]\n",
      "Accuracy: 63.58%\n"
     ]
    }
   ],
   "source": [
    "# Convert true_labels and predicted_labels to numpy arrays\n",
    "true_labels_np = np.array(true_labels)\n",
    "predicted_labels_np = np.array(predicted_labels)\n",
    "\n",
    "# print(type(true_labels_np))\n",
    "# print(true_labels_np)\n",
    "# print(predicted_labels_np)\n",
    "# print(len(true_labels_np))\n",
    "# print(len(predicted_labels_np))\n",
    "\n",
    "\n",
    "nan_indices = np.isnan(true_labels_np)\n",
    "print(nan_indices)\n",
    "\n",
    "# Remove NaN values from both true_labels_np and predicted_labels_np\n",
    "true_labels_np = true_labels_np[~nan_indices]\n",
    "predicted_labels_np = predicted_labels_np[~nan_indices]\n",
    "\n",
    "\n",
    "# print(\"After removing\")\n",
    "# print(len(true_labels_np))\n",
    "# print(len(predicted_labels_np))\n",
    "\n",
    "\n",
    "# Convert string labels to numeric values\n",
    "label_mapping = {\"positive\": 2, \"neutral\": 1, \"negative\": 0}\n",
    "\n",
    "# Convert string labels to numeric values for predicted_labels_np\n",
    "predicted_labels_np = np.array([label_mapping.get(label, label) for label in predicted_labels_np])\n",
    "print(predicted_labels_np)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels_np, predicted_labels_np)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
